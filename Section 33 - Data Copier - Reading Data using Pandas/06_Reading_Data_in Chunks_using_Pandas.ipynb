{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc8c0b4",
   "metadata": {},
   "source": [
    "## Reading Data in Chunks using Pandas\n",
    "When size of data is huge, it is recommended to read the data in chunks, then process and then to write to the target. Let us understand how to read the data from JSON files using Pandas in chunks.\n",
    "\n",
    "```python\n",
    "fp = '/Users/itversity/Projects/Internal/bootcamp/data-copier/data/retail_db_json/order_items/part-r-00000-6b83977e-3f20-404b-9b5f-29376ab1419e'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Here is the piece of code to read the content of the file as reader.\n",
    "json_reader = pd.read_json(fp, lines=True, chunksize=1000)\n",
    "\n",
    "# Here is the piece of code to read each chunk as Dataframe.\n",
    "for idx, df in enumerate(json_reader):\n",
    "  print(f'Number of records in chunk with index {idx} is {df.shape[0]}')\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
